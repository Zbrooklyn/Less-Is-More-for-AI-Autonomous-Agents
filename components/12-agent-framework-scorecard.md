# Agent Framework Scorecard

## Definition

A comparative analysis of every major AI agent tool and framework — scored against the 10-component framework defined in "Less Is More for AI Autonomous Agents." Each tool gets rated on each component (HAVE / PARTIAL / PRIMITIVE / DON'T HAVE), revealing where each tool is strong, where it's weak, and who's closest to the full 100%.

## Purpose

The AI agent space is crowded and moving fast. Claude Code, Cursor, Copilot, Devin, OpenHands, Windsurf, Aider, Continue, Cline — every tool claims to be the most capable. But without a consistent scoring framework, comparing them is just vibes and marketing.

This document applies the same 10-component lens to every major player. It answers: who's actually closest to a fully autonomous agent? Where is each tool investing? Where are the market gaps that no one is filling? And most importantly — which tool should you use for which type of work?

## What It Will Cover

- Every major AI coding agent scored on all 10 components
- A comparison matrix showing strengths and weaknesses at a glance
- Per-tool analysis — what each tool does well and where it falls short
- Market gap analysis — which components are underserved across the entire ecosystem
- Best-in-class picks — which tool leads on each specific component
- Trajectory analysis — which tools are moving fastest toward full autonomy
- The honest answer to "which one should I use?"

## Tools to Score

- Claude Code (Anthropic)
- Cursor (Anysphere)
- GitHub Copilot / Copilot Workspace
- Devin (Cognition)
- OpenHands (formerly OpenDevin)
- Windsurf (Codeium)
- Aider
- Continue
- Cline
- Amazon Q Developer
- Google Jules / Gemini Code Assist
- Codex CLI (OpenAI)
